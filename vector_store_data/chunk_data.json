[
  {
    "id": "48423a8f-9346-4df6-8922-c50759b037d0",
    "content": "Coding Round Problem Statement\n\nTask Title:\nAgentic RAG Chatbot for Multi-Format Document QA using Model Context Protocol (MCP)\n\nProblem Statement:\nYou are required to build an agent-based Retrieval-Augmented Generation (RAG) chatbot that can answer user questions using uploaded documents of various formats. Your architecture must follow an agentic structure and should incorporate Model Context Protocol (MCP) as the mechanism for communication between agents and/or agents ‚Üî LLMs.",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 0
    }
  },
  {
    "id": "7aeaa813-eb34-46d3-8ef4-2f136b3b2484",
    "content": "Core Functional Requirements\nYour solution must:\n1. Support Uploading & Parsing of Diverse Document Formats:\n   - PDF\n   - PPTX\n   - CSV\n   - DOCX\n   - TXT / Markdown\n2. Agentic Architecture (minimum 3 agents):\n   - IngestionAgent: Parses & preprocesses documents.\n   - RetrievalAgent: Handles embedding + semantic retrieval.\n   - LLMResponseAgent: Forms final LLM query using retrieved context and generates answer.\n3. Use Model Context Protocol (MCP):",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 1
    }
  },
  {
    "id": "4dabc7ce-a811-4346-9876-451405f6c185",
    "content": "3. Use Model Context Protocol (MCP):\n   - Each agent must send/receive messages using structured MCP-like context objects, such as:\n     {\n       \"sender\": \"RetrievalAgent\",\n       \"receiver\": \"LLMResponseAgent\",\n       \"type\": \"CONTEXT_RESPONSE\",\n       \"trace_id\": \"abc-123\",\n       \"payload\": {\n         \"top_chunks\": [\"...\", \"...\"],\n         \"query\": \"What are the KPIs?\"\n       }\n     }\n   - You can implement MCP using in-memory messaging, REST, or pub/sub.\n4. Vector Store + Embeddings",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 2
    }
  },
  {
    "id": "6655dfd8-7dea-4522-b0c8-8d19735f1652",
    "content": "4. Vector Store + Embeddings\n   - Use any embeddings (OpenAI, HuggingFace, etc.)\n   - Use a vector DB (FAISS, Chroma, etc.)\n5. Chatbot Interface (UI)\n   - Allow users to:\n     - Upload documents\n     - Ask multi-turn questions\n     - View responses with source context\n   - Use any UI framework: Streamlit, React, Angular, Flask, etc.",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 3
    }
  },
  {
    "id": "37e97e55-b2d5-4e32-980f-ef8b001c74a8",
    "content": "Deliverables\n1. GitHub Repository\n   - Include:\n     - Well-organized code\n     - Clear README.md with setup instructions\n2. PPT Presentation\n   - Slide deck (3‚Äì6 slides) must include:\n     - Agent-based architecture with MCP integration\n     - System flow diagram (with message passing)\n     - Tech stack used\n     - UI screenshots of working app\n     - Challenges Faced while doing the project\n     - (Optional) future scope / improvements\n3. Submission\n   - Share:",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 4
    }
  },
  {
    "id": "b635c00e-66ce-4bb7-a0fd-f634c2579a5e",
    "content": "- (Optional) future scope / improvements\n3. Submission\n   - Share:\n     - Public GitHub repository link\n     - Architecture PPT (PDF or PPTX) [ To be included in the GitRepo Itself]\n     - Include a Video for 5 mins where 1 min give the application demo, 2 min architecture and flow explanation, 2 min code explanation. (Its optional to show face)",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 5
    }
  },
  {
    "id": "85fd4af4-6538-4507-be33-4682bf087d11",
    "content": "Sample Workflow (Message Passing with MCP)\nUser uploads: sales_review.pdf, metrics.csv\nUser: \"What KPIs were tracked in Q1?\"\n-> UI forwards to CoordinatorAgent\n-> Coordinator triggers: üî∏ IngestionAgent -> parses documents\nüî∏ RetrievalAgent -> finds relevant chunks\nüî∏ LLMResponseAgent -> formats prompt & calls LLM\n-> Chatbot shows answer + source chunks",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 6
    }
  },
  {
    "id": "d0dae6b8-a287-4dfb-81f3-b412a2a2ec57",
    "content": "MCP message example:\n{\n  \"type\": \"RETRIEVAL_RESULT\",\n  \"sender\": \"RetrievalAgent\",\n  \"receiver\": \"LLMResponseAgent\",\n  \"trace_id\": \"rag-457\",\n  \"payload\": {\n    \"retrieved_context\": [\"slide 3: revenue up\", \"doc: Q1 summary...\"],\n    \"query\": \"What KPIs were tracked in Q1?\"\n  }\n}\n\nHi there!",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 7
    }
  },
  {
    "id": "6d77f993-4867-44f2-994a-f7ba32118af4",
    "content": "Hi there!\n\nThanks for sharing your details through the Google Form. After reviewing your background, tech stack, and GitHub projects, we‚Äôre excited to let you know that you‚Äôve been shortlisted for the next round of our hiring process!\n\nCoding Test:\nhttps://drive.google.com/file/d/1dLGeruLl6NaS2BrT_wqaHV4nlj4tEGtp/view",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 8
    }
  },
  {
    "id": "2666abfa-c5b2-4301-b634-bada8a6a89a1",
    "content": "Coding Test:\nhttps://drive.google.com/file/d/1dLGeruLl6NaS2BrT_wqaHV4nlj4tEGtp/view\n\nGo through the project requirements carefully. Once done, upload your solution to a public GitHub repo and submit the details using the form below within 72 hours of receiving this email.\n\nSubmit Your Project:\nhttps://forms.gle/Etga1U1xozRbw9qW9",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 9
    }
  },
  {
    "id": "8dae5b03-c087-4e8c-8337-0c1025aff456",
    "content": "Submit Your Project:\nhttps://forms.gle/Etga1U1xozRbw9qW9\n\nYour Submission Should Include:\n‚Ä¢ A GitHub repo with clean, well-commented code and a helpful README.md\n‚Ä¢ An Architecture presentation (PDF or PPTX) ‚Äì upload this to the repo and the form\n‚Ä¢ A short 5-minute video:\no 1 min: App demo\no 2 min: System architecture & flow\no 2 min: Code walkthrough\n‚Ä¢ A few lines on any challenges you faced or improvements you‚Äôd suggest (can go in the README or the form)\n\nDeadline: 31st july 2025",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 10
    }
  },
  {
    "id": "61032a2e-786c-4f4a-a8b4-32b74a067049",
    "content": "Deadline: 31st july 2025\n\nWe‚Äôre looking forward to seeing your creativity and problem-solving in action. Good luck and happy coding!\n\n--\nBest Regards",
    "metadata": {
      "source_filename": "coding_round_problem_statement.txt",
      "original_filepath": "docs\\coding_round_problem_statement.txt",
      "file_type": "txt",
      "chunk_index": 11
    }
  },
  {
    "id": "3f9bca94-8987-477e-9818-05f74e28cb59",
    "content": "Metric    Q1_2025    Q2_2025    Q3_2025    Q4_2025\n    Total Revenue (USD) 15200000.0 16500000.0 17800000.0 19000000.0\n          New Customers     5100.0     5500.0     5800.0     6200.0\nCustomer Churn Rate (%)        8.0        7.5        7.0        6.8\n             ARPU (USD)     2980.0     3000.0     3050.0     3100.0\n     Sales Cycle (Days)       45.0       42.0       40.0       38.0\n             CLTV (USD)    12000.0    12500.0    13000.0    13500.0",
    "metadata": {
      "source_filename": "metrics_dashboard_data.csv",
      "original_filepath": "docs\\metrics_dashboard_data.csv",
      "file_type": "csv",
      "chunk_index": 0
    }
  },
  {
    "id": "a950edd7-0467-402c-a9ae-66c114cd439a",
    "content": "CLTV (USD)    12000.0    12500.0    13000.0    13500.0\n  Marketing Spend (USD)  1500000.0  1600000.0  1700000.0  1800000.0\n         Website Visits   500000.0   550000.0   600000.0   650000.0\n    Conversion Rate (%)        3.5        3.7        3.8        4.0",
    "metadata": {
      "source_filename": "metrics_dashboard_data.csv",
      "original_filepath": "docs\\metrics_dashboard_data.csv",
      "file_type": "csv",
      "chunk_index": 1
    }
  },
  {
    "id": "330870f3-85ea-4e53-bfad-e5b0817779a0",
    "content": "SAMPLE\n\n\nSlide 1: Title Slide\nTitle: Agentic RAG Chatbot Project\nSubtitle: Overview & Architecture\nAuthor: Your Name / Team\n\nSlide 2: Problem Statement\n\"Build an agent-based RAG chatbot.\"\n\"Multi-format document Q&A.\"\n\"Key: Agentic structure + Model Context Protocol (MCP).‚Äú\n\nSlide 3: Core Requirements\nDocument Parsing (PDF, PPTX, CSV, DOCX, TXT)\n3+ Agents (Ingestion, Retrieval, LLMResponse)\nMCP for inter-agent comms\nVector Store (FAISS) + Embeddings (Gemini)\nChatbot UI (Streamlit)",
    "metadata": {
      "source_filename": "presentation_notes.pptx",
      "original_filepath": "docs\\presentation_notes.pptx",
      "file_type": "pptx",
      "chunk_index": 0
    }
  },
  {
    "id": "47a2b45f-b27d-4c34-b548-8c066ee6c1e6",
    "content": "Slide 4: Agentic Architecture (Conceptual Diagram Text)\nUser -> UI -> Coordinator Agent\nCoordinator -> IngestionAgent (Parses Docs)\nIngestionAgent -> RetrievalAgent (Embeds & Stores)\nRetrievalAgent -> LLMResponseAgent (Retrieves Context)\nLLMResponseAgent -> LLM (Generates Answer)\nLLM -> LLMResponseAgent -> UI -> User",
    "metadata": {
      "source_filename": "presentation_notes.pptx",
      "original_filepath": "docs\\presentation_notes.pptx",
      "file_type": "pptx",
      "chunk_index": 1
    }
  },
  {
    "id": "affe6155-cd8b-49b6-8586-ee564bf93a25",
    "content": "Slide 5: Model Context Protocol (MCP) Example\n{ \"sender\": \"RetrievalAgent\", \"receiver\": \"LLMResponseAgent\", \"type\": \"CONTEXT_RESPONSE\", \"payload\": { \"top_chunks\": [...], \"query\": \"...\" } }\nPurpose: Standardized, structured communication.\n\nSlide 6: Deliverables\nGitHub Repo (Code, README)\nPPT Presentation (this!)\n5-min Video Demo/Explanation\nDeadline: July 31, 2025",
    "metadata": {
      "source_filename": "presentation_notes.pptx",
      "original_filepath": "docs\\presentation_notes.pptx",
      "file_type": "pptx",
      "chunk_index": 2
    }
  },
  {
    "id": "2755e461-8004-4155-b232-464de89bbf42",
    "content": "Project Development Milestones - Agentic RAG Chatbot Version: 1.0 Date: July 31, 2025 Phase 1: Core Component Development (Completed Q2 2025) - Document Parsing (PDF, DOCX, TXT, CSV, PPTX) - Text Chunking Module - Embedding Generation Integration (Gemini API) - FAISS Vector Store Implementation Phase 2: Agentic Architecture & MCP (Target Q3 2025) - Design and implement IngestionAgent - Design and implement RetrievalAgent - Design and implement LLMResponseAgent - Implement Model Context Protocol",
    "metadata": {
      "source_filename": "project_milestones.docx",
      "original_filepath": "docs\\project_milestones.docx",
      "file_type": "docx",
      "chunk_index": 0
    }
  },
  {
    "id": "13531676-775b-4009-9b9a-c6c4c8ccd4fe",
    "content": "implement RetrievalAgent - Design and implement LLMResponseAgent - Implement Model Context Protocol for inter-agent communication - Define core message types (CONTEXT_REQUEST, CONTEXT_RESPONSE, etc.) - Establish in-memory message bus (or alternative) Phase 3: Chatbot UI & Integration (Target Q3/Q4 2025) - Develop Streamlit (or chosen UI framework) interface - Enable document upload functionality - Implement multi-turn conversation history - Display retrieved source context alongside answers",
    "metadata": {
      "source_filename": "project_milestones.docx",
      "original_filepath": "docs\\project_milestones.docx",
      "file_type": "docx",
      "chunk_index": 1
    }
  },
  {
    "id": "5ebd13b9-bb26-4eec-b8ab-38f0183b971b",
    "content": "- Implement multi-turn conversation history - Display retrieved source context alongside answers Phase 4: Testing & Refinement (Ongoing) - Unit tests for all modules - Integration tests for agent workflows - Performance optimization - User Acceptance Testing (UAT) Future Scope (Post-Deadline): - Advanced Reranking mechanisms - Hybrid search (keyword + semantic) - Support for more complex data types (e.g., tables directly in LLM context) - User feedback loop for answer quality - Deployment to",
    "metadata": {
      "source_filename": "project_milestones.docx",
      "original_filepath": "docs\\project_milestones.docx",
      "file_type": "docx",
      "chunk_index": 2
    }
  },
  {
    "id": "11c672f1-bff0-4ffe-884d-8e6ef4505bb5",
    "content": "(e.g., tables directly in LLM context) - User feedback loop for answer quality - Deployment to cloud platform",
    "metadata": {
      "source_filename": "project_milestones.docx",
      "original_filepath": "docs\\project_milestones.docx",
      "file_type": "docx",
      "chunk_index": 3
    }
  },
  {
    "id": "9222e694-c929-484c-a285-c5e6010e077b",
    "content": "Q1 Sales Performance Summary - Confidential Prepared for: Executive Leadership Team \nDate: July 31, 2025 Overview: Q1 saw robust growth across key product lines, exceeding \ninternal forecasts. Total Revenue: $15.2 Million (20% increase over Q4 last year) New  \nCustomer Acquisition: 5,100 (15% increase) Average Deal Size: $2,980 Customer Retention \nRate: 92% Key Performance Indicators (KPIs) Tracked: - Total Revenue (MRR for recurring,",
    "metadata": {
      "source_filename": "q1_sales_report_summary.pdf",
      "original_filepath": "docs\\q1_sales_report_summary.pdf",
      "file_type": "pdf",
      "chunk_index": 0
    }
  },
  {
    "id": "a5c7fdae-90b0-40e1-84a8-7b33abd4bc57",
    "content": "Rate: 92% Key Performance Indicators (KPIs) Tracked: - Total Revenue (MRR for recurring, \nARR for annual contracts) - New Customer Acquisition Rate - Customer Churn Rat e - \nAverage Revenue Per User (ARPU) - Sales Cycle Length - Customer Lifetime Value (CLTV) \nRegional Performance: - North America: Strongest growth, particularly in SaaS \nsubscriptions. - Europe: Steady performance, slight increase in professional services. -",
    "metadata": {
      "source_filename": "q1_sales_report_summary.pdf",
      "original_filepath": "docs\\q1_sales_report_summary.pdf",
      "file_type": "pdf",
      "chunk_index": 1
    }
  },
  {
    "id": "37516357-6268-4f7c-ade7-45a6293bb18c",
    "content": "subscriptions. - Europe: Steady performance, slight increase in professional services. - \nAPAC: Emerging market, significant potential for Q2 expansion. Challenges: - Increased \ncompetition in SaaS market. - Longer sales cycles for enterprise clients. Opportunities: - \nExpansion into new APAC territories. - Launch of Product X in Q2. - Strategic  partnerships \nto enhance market reach. Recommendations: - Allocate increased marketing budget to",
    "metadata": {
      "source_filename": "q1_sales_report_summary.pdf",
      "original_filepath": "docs\\q1_sales_report_summary.pdf",
      "file_type": "pdf",
      "chunk_index": 2
    }
  },
  {
    "id": "639d24a7-8027-4ab7-91b4-10347967aabe",
    "content": "to enhance market reach. Recommendations: - Allocate increased marketing budget to \nNorth America and APAC. - Invest in sales training to shorten enterprise sales cycles. - \nExplore AI -driven lead generation tools.",
    "metadata": {
      "source_filename": "q1_sales_report_summary.pdf",
      "original_filepath": "docs\\q1_sales_report_summary.pdf",
      "file_type": "pdf",
      "chunk_index": 3
    }
  },
  {
    "id": "c9f43bd6-9375-4ddd-b0e3-b691d5a8a2b3",
    "content": "JOSHUA PETER \nPOLAPRAYIL\nDetails\nEravimangalam P O\nKottayam, Kerala, 686613 \nIndia\n+91 8281158864\njosh19peter96@gmail.com\nLinks\nGitHub\nLinkedIn\nLanguages\nEnglish\nMalayalam\nHindi\nTamilProfile\nJunior software engineer with a Master‚Äôs in Big Data Analytics & AI and hands-on \nexperience in deep learning (CNN, RNN, BioBERT), data pipelines (Airflow, dbt, \nBigQuery), and fullstack systems (NestJS, TypeScript, React, PostgreSQL). Skilled",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 0
    }
  },
  {
    "id": "f333babf-f53a-4430-95db-e6b5614fa740",
    "content": "BigQuery), and fullstack systems (NestJS, TypeScript, React, PostgreSQL). Skilled \nin deploying applications on AWS/GCP with Docker and CI/CD, and experienced \nin enterprise development with C#, .NET, and SQL Server. Strong foundation \nin Python, SQL, and practical problem-solving across AI, data, and product \nengineering.\nSkills\nLanguages:\nPython, TypeScript, C#, SQL, JavaScript, Shell, PowerShell\nArtificial Intelligence & Machine Learning:",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 1
    }
  },
  {
    "id": "42b17976-7a6a-4868-b2aa-3a53afa86044",
    "content": "Artificial Intelligence & Machine Learning:\nGenerative AI, LLM Integration, Prompt Engineering, Multi-Agent Systems, \nRAG (Retrieval-Augmented Generation) Deep Learning (CNN, RNN, Bi-LSTM, \nTransformers), NLP , Computer Vision (Object Detection, Tracking), Machine \nLearning LangChain, LangGraph, PyTorch, TensorFlow, Google Gemma API, \nChromaDB, YOLO, OpenCV Model Optimization, Hyperparameter Tuning, Text \nGeneration, Multi-Modal AI\nData Engineering & Analytics:",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 2
    }
  },
  {
    "id": "8f1a231b-dc7e-409b-a361-0509eea06ea8",
    "content": "Generation, Multi-Modal AI\nData Engineering & Analytics:\nApache Spark, Kafka, Airflow, dbt, ETL Pipelines, Data Manipulation (NumPy, \nPandas) Google Cloud Storage (GCS), BigQuery, SQL Server Integration Services \n(SSIS)\nDevOps & Cloud:\nDocker, AWS (EC2, RDS, S3), GCP , GitHub Actions, CI/CD, Terraform\nBackend & Database Stack:\nNestJS, .NET, FastAPI, PostgreSQL, MS SQL Server, Prisma ORM SQL Optimization, \nDatabase Performance Tuning\nFrontend & UI:\nReact, Streamlit, Bootstrap",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 3
    }
  },
  {
    "id": "651b20d4-bce1-4cef-8d1a-4685c1c5e8e2",
    "content": "Database Performance Tuning\nFrontend & UI:\nReact, Streamlit, Bootstrap\nTools & Methodologies:\nPostman (API Testing), VS Code, Jira, Power BI Agile-Scrum, System Design, API \nDesign, Software Testing, Real-time Systems\nEmployment History\nAI Intern / Backend Contributor, Hubinit (Fuuss), Amsterdam, \nNetherlands - Remote\nA U G U S T  2 0 2 4  ‚Äî  M AY  2 0 2 5 \n‚Ä¢Built and refactored core backend services using NestJS, TypeScript, \nPostgreSQL, and Prisma ORM, focusing on modules like AuthService,",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 4
    }
  },
  {
    "id": "479ee3e7-32ab-4ea0-8c56-0be91d9765bf",
    "content": "PostgreSQL, and Prisma ORM, focusing on modules like AuthService, \nCardService, and UserService to ensure system stability and scalability.\n‚Ä¢Took ownership of early product definition, mapping the full data model, \nconducting research, and presenting MVP specifications that shaped the \nproject direction in collaboration with the CEO and cross-functional teams.\n‚Ä¢Contributed to implementing and refining role-based access control (RBAC)",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 5
    }
  },
  {
    "id": "967a1922-1500-4a2b-8e1e-ad9b85ec677a",
    "content": "‚Ä¢Contributed to implementing and refining role-based access control (RBAC) \nfor multi-role dashboards (Super Admin, Business Owner, Customer).\n‚Ä¢Collaborated with DevOps on deploying Dockerized services and managing \ninfrastructure using AWS EC2, RDS, S3, CloudWatch, and Amazon SES, while \ncontributing to CI/CD scripting and deployment workflows.\n‚Ä¢Investigated and helped mitigate a bot attack, analyzing AWS RDS PostgreSQL \nlogs, implementing dynamic rate limiting, and coordinating reCAPTCHA",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 6
    }
  },
  {
    "id": "e272e4a3-7f5a-44b7-892b-43d047321d37",
    "content": "logs, implementing dynamic rate limiting, and coordinating reCAPTCHA \nintegration with frontend and management.‚Ä¢Facilitated seamless frontend-backend integration for the React + TypeScript \nfrontend, defining API contracts and supporting features like wallet and \nloyalty card management.\n‚Ä¢Maintained and extended automated Postman API test suites, ensuring route \nstability and reducing regressions.\n‚Ä¢Collaborated daily in a Scrum-based Agile team, and directly advised the CEO",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 7
    }
  },
  {
    "id": "6639ad3e-0ec2-4839-98fe-6ec518218b62",
    "content": "‚Ä¢Collaborated daily in a Scrum-based Agile team, and directly advised the CEO \nand HR on developer morale, team retention, and internal challenges during \norganizational transitions.\nAI Intern, RoshAI, Kerala, India\nM A R C H  2 0 2 5  ‚Äî  M AY  2 0 2 5 \n‚Ä¢Contribute to the development of computer vision models for autonomous \ndriving and ADAS, focusing on image-based object detection tasks.\n‚Ä¢Curate, label, and refine image datasets using Azure Custom Vision to support",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 8
    }
  },
  {
    "id": "7aab3eab-e61d-46c0-be25-b5d93c4ac9aa",
    "content": "‚Ä¢Curate, label, and refine image datasets using Azure Custom Vision to support \ntraining pipelines and improve detection accuracy.\n‚Ä¢Customized YOLOv7 for detecting vehicle lights (high/low beam) and \npedestrian crosswalks by implementing early stopping, anchor box tuning, \nimage-quality-based sample weighting, and advanced hyperparameter search.\n‚Ä¢Integrated additional training hooks into YOLOv7 to enhance convergence \nand training stability across varied lighting conditions.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 9
    }
  },
  {
    "id": "3d1f6aa3-c58c-4ac0-9116-7adbdded14ca",
    "content": "and training stability across varied lighting conditions.\n‚Ä¢Processed Tata FleetEdge telemetry data for driver behavior analysis, \nengineered features for smoothness scoring, and built an API-ready FastAPI \npipeline for real-time insights.\n‚Ä¢Collaborate with cross-functional AI and data teams to iterate on models and \nsupport deployment-readiness for production environments.\nData Science Intern, Irohub Infotech, Kerala, India\nJ U N E  2 0 2 4  ‚Äî  J A N U A RY  2 0 2 5",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 10
    }
  },
  {
    "id": "c9fd5bc7-f085-4193-925d-01d51ae60537",
    "content": "Data Science Intern, Irohub Infotech, Kerala, India\nJ U N E  2 0 2 4  ‚Äî  J A N U A RY  2 0 2 5 \n‚Ä¢Gained experience in NLP , machine learning (regression, clustering), Trees, \ndeep learning (ANN, CNN, RNN, bi-LSTM), and integrated solutions with \nOpenCV.\n‚Ä¢Contributed to real-world projects and delivered results through regular \nassessments and hands-on tasks.\n‚Ä¢Gained hands on experience with tools and platforms such as Streamlit, Excel, \nPower BI, and SQL.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 11
    }
  },
  {
    "id": "1368b142-19a6-4b34-964a-37f9e644f30c",
    "content": "‚Ä¢Gained hands on experience with tools and platforms such as Streamlit, Excel, \nPower BI, and SQL.\n‚Ä¢Developed data manipulation and visualization skills using NumPy, Pandas, \nMatplotlib, and Seaborn.\nJunior Software Engineer, Permanent TSB, Dublin, Ireland\nJ U LY  2 0 2 2  ‚Äî  J A N U A RY  2 0 2 3 \n‚Ä¢Took over a critical loan assessment application project during its testing and \ndeployment phase, supporting the migration of a legacy Excel/Visual Basic",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 12
    }
  },
  {
    "id": "4357e5d0-1d04-4770-a7be-f6bcac89d460",
    "content": "deployment phase, supporting the migration of a legacy Excel/Visual Basic \nsolution to a modern C#/.NET web application with a Bootstrap frontend.\n‚Ä¢Led resolution of underwriter-reported issues by triaging bugs, tracing logic \nflow across C# controllers, frontend input handlers, and SQL interactions, and \ndelivering fixes for functional and UI-related defects.\n‚Ä¢Used Jira as a central coordination tool to log, track, and document all bug",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 13
    }
  },
  {
    "id": "a18e4e14-0316-4c70-afca-67c8c7b80488",
    "content": "‚Ä¢Used Jira as a central coordination tool to log, track, and document all bug \nresolutions, code locations, logic changes, and status updates‚Äîstreamlining \nfeedback loops between testers, analysts, and technical leads.\n‚Ä¢Worked closely with underwriters to understand business use cases tied \nto Standard Financial Statement (SFS) data, ensuring accurate handling of \ntreatment workflows, budgeting recommendations, and supervisor reviews.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 14
    }
  },
  {
    "id": "858e887e-2fc2-431b-8e6c-3f704bcf67dd",
    "content": "treatment workflows, budgeting recommendations, and supervisor reviews.\n‚Ä¢Engaged with a large-scale SQL Server backend‚Äîanalyzing how stored \nprocedures, functions, and data pipelines (owned by the business team) \ninfluenced application logic and user experience.\n‚Ä¢Delivered the application‚Äôs initial release milestone by proactively addressing \na wide range of bugs and feature requests, despite limited documentation and",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 15
    }
  },
  {
    "id": "eff45f3b-5fc6-445f-b524-5e7746306a4a",
    "content": "a wide range of bugs and feature requests, despite limited documentation and \nabrupt developer handoffs.Junior Database Developer, Datafloat Technologies India Pvt. Ltd., \nKerala, India\nJ U N E  2 0 1 9  ‚Äî  A U G U S T  2 0 2 0 \n‚Ä¢Developed and maintained SQL Server databases for leading U.S.-based real \nestate firms, while gaining experience with Windows Server environments.\n‚Ä¢Performed ETL operations across staging and production servers using",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 16
    }
  },
  {
    "id": "e05e64e5-e375-452a-80b2-274ae452d3a5",
    "content": "‚Ä¢Performed ETL operations across staging and production servers using \nRETS/RESO APIs, PowerShell, batch scripts, and stored procedures.\n‚Ä¢Automated routine database tasks with SQL Server Agent and Windows Task \nScheduler, improving process efficiency.\n‚Ä¢Collaborated with front-end and metadata teams to meet project deadlines \nand enhance delivery outcomes.\n‚Ä¢Created routine reports using SSRS and XML scripting; conducted data \nmigration across servers using SSIS.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 17
    }
  },
  {
    "id": "cc46c157-f5ea-4755-a6ed-2f5855fa0dd2",
    "content": "migration across servers using SSIS.\n‚Ä¢Used Jira to manage tickets and provide timely resolutions for \ndatabase-related issues.\n‚Ä¢Optimized SQL queries and handled debugging, performance tuning, and \ndeadlock resolution.\n‚Ä¢Mentored and onboarded new joiners, helping them understand tools, \nworkflows, and company practices.\nEducation\nMaster of Science: Big Data Analytics and Artificial Intelligence, \nLetterkenny Institute of Technology (Now Atlantic Technological",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 18
    }
  },
  {
    "id": "d365e669-6b0b-4430-9064-6c2ca5864462",
    "content": "Letterkenny Institute of Technology (Now Atlantic Technological \nUniversity - ATU), Letterkenny, Donegal, Ireland\nS E P T E M B E R  2 0 2 0  ‚Äî  O C T O B E R  2 0 2 1 \n‚Ä¢Completed an intensive program focused on examining, interpreting, and \nleveraging large datasets, and enabling intelligent decision-making systems.¬†\n‚Ä¢Gained expertise in advanced data analytics, machine learning, and artificial \nintelligence, with an emphasis on practical application and real-world \nprojects.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 19
    }
  },
  {
    "id": "a4869699-2e09-43fe-8737-36f6ad3c9e6a",
    "content": "intelligence, with an emphasis on practical application and real-world \nprojects.¬†\n‚Ä¢Core Modules: Natural Language Processing, Computer Vision, Data \nPre-Processing, Machine Learning, Deep Learning, Hadoop, Apache Spark, \nAmazon AWS, Databricks, Linux Console, Kafka Confluent, and KSQL.\nBachelor of Engineering: Computer Science and Engineering, Loyola \nInstitute of Technology and Science ‚Äê Anna University, Nagercoil, \nTamil Nadu, India\nA U G U S T  2 0 1 3  ‚Äî  M AY  2 0 1 7",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 20
    }
  },
  {
    "id": "c05d342d-e875-49c9-ad4e-22d27011c741",
    "content": "Tamil Nadu, India\nA U G U S T  2 0 1 3  ‚Äî  M AY  2 0 1 7 \n‚Ä¢Acquired foundational knowledge in computer science and engineering \nprinciples, focusing on the design, development, and analysis of computing \nsolutions.¬†\n‚Ä¢Developed strong problem-solving and analytical skills, applying software \nengineering methodologies for robust system development.¬†\n‚Ä¢Prepared to adapt to emerging information and communication technologies \nand innovate solutions for complex challenges.\nProjects",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 21
    }
  },
  {
    "id": "de3327f7-5107-43ff-9419-a4be607f57cd",
    "content": "and innovate solutions for complex challenges.\nProjects\nAI Assistant Hub (Multi-Agent System)\nJ U N E  2 0 2 5 \n‚Ä¢Architected and developed a modular multi-agent AI system using LangGraph \nto dynamically orchestrate 6+ specialized agents (e.g., Real Estate, RAG-based \nQA, Summarizer, Translation), enabling diverse and complex query handling.\n‚Ä¢Engineered advanced prompt engineering strategies to simulate sophisticated \nfunction chaining and reasoning with Google Gemma API and other",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 22
    }
  },
  {
    "id": "b583d964-0ed5-4609-8abd-8c273dbb44b9",
    "content": "function chaining and reasoning with Google Gemma API and other \nopen-source LLMs, effectively overcoming limitations in native tool-calling.\n‚Ä¢Integrated a Retrieval-Augmented Generation (RAG) system with ChromaDB \nand external APIs (e.g., DuckDuckGo Search) for context-aware information \nretrieval and real-time data access.‚Ä¢Designed and implemented an intuitive Streamlit UI featuring chat history, \nfile uploads, and transparent reasoning logs, significantly enhancing user",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 23
    }
  },
  {
    "id": "586b79ba-e2fd-442f-aa1e-e402461d94d6",
    "content": "file uploads, and transparent reasoning logs, significantly enhancing user \nexperience and system interpretability.\n‚Ä¢Demonstrated proficiency in LangChain and LangGraph by building a robust, \nextensible agentic architecture entirely with free and open-source resources.\nGitHub ¬†¬†¬†¬†| ¬†¬†¬†¬†Render\nMedical AI Chatbot with BioBERT-Transformer Architecture\nJ U N E  2 0 2 5 \n‚Ä¢Developed a medical Q&A chatbot using a custom BioBERT-based encoder",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 24
    }
  },
  {
    "id": "8de089ee-0c25-488e-b014-510a18ca3d51",
    "content": "J U N E  2 0 2 5 \n‚Ä¢Developed a medical Q&A chatbot using a custom BioBERT-based encoder \nand Transformer decoder architecture, optimized for clinical language \nunderstanding.¬†\n‚Ä¢Trained on 260K+ real-world doctor‚Äìpatient conversations using \nTFRecord-like caching, AMP training, label smoothing, and OneCycleLR \nscheduling in PyTorch.¬†\n‚Ä¢Implemented a modular pipeline with early stopping, gradient accumulation, \ncheckpoint recovery, and sample logging for transparent debugging and \ntuning.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 25
    }
  },
  {
    "id": "5e2f9d27-2bde-4943-b2e3-0fbdbadc3ddc",
    "content": "checkpoint recovery, and sample logging for transparent debugging and \ntuning.¬†\n‚Ä¢Built dual Streamlit interfaces using Beam Search and Nucleus Sampling to \nsupport controlled and diverse generation modes.\nGitHub\nMSc Dissertation - A novel racism detection model aiming to \nminimize bias \nA U G U S T  2 0 2 1 \n‚Ä¢Designed and implemented multi-modal architecture.¬†\n‚Ä¢Built a network based on BERT Embeddings and multi-branched CNNs on top",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 26
    }
  },
  {
    "id": "6d773a02-af8b-41a1-9700-b01e3aacee60",
    "content": "‚Ä¢Built a network based on BERT Embeddings and multi-branched CNNs on top \nof bi-LSTM and a final classification layer for text classification.¬†\n‚Ä¢Used a dense neural network for demographic and contextual classification \n(numerical) completing the architecture.\nGithub\nHeart Attack Prediction in Indonesia - Capstone Project | Data \nEngineering Zoomcamp\nM A R C H  2 0 2 5 \n‚Ä¢Designed and deployed a scalable data pipeline on GCP using Terraform, GCS, \nBigQuery, dbt, and Apache Airflow.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 27
    }
  },
  {
    "id": "4d05da3b-4774-4e2c-a26d-da6d7d1cfb66",
    "content": "BigQuery, dbt, and Apache Airflow.¬†\n‚Ä¢Automated ingestion, transformation, and visualization of heart attack data.¬†\n‚Ä¢Built Power BI dashboards and CI/CD workflows with GitHub Actions for \nreal-time insights.\nProject GitHub ¬†¬†¬†¬†| ¬†¬†¬†¬†Course ¬†GitHub\nElectric Vehicle Market Analysis & Predictive Modeling\nD E C E M B E R  2 0 2 4 \n‚Ä¢Analyzed EV market trends and built predictive models for future \ncharacteristics through 2030.\n‚Ä¢Cleaned and engineered features using regex, imputation, and correlation",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 28
    }
  },
  {
    "id": "2779e7db-0e97-45e3-a2ed-d01784d734a8",
    "content": "‚Ä¢Cleaned and engineered features using regex, imputation, and correlation \nanalysis.\n‚Ä¢Built a multi-output regression model in TensorFlow to predict EV price and \nrange.\n‚Ä¢Tuned hyperparameters via Bayesian optimization with Keras Tuner.\n‚Ä¢Applied DBSCAN clustering with full preprocessing pipelines to identify \nmarket segments.\n‚Ä¢Visualized findings with time series, heatmaps, and 3D cluster plots using \nMatplotlib and Seaborn.\nGitHubFake News Detection System\nD E C E M B E R  2 0 2 4",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 29
    }
  },
  {
    "id": "6b24c110-9980-4e77-9e34-a1998eea6a0c",
    "content": "Matplotlib and Seaborn.\nGitHubFake News Detection System\nD E C E M B E R  2 0 2 4 \n‚Ä¢Built an end-to-end fake news classifier using NLP and optimized ML models.¬†\n‚Ä¢Engineered text preprocessing with NLTK (tokenization, lemmatization, \ncustom cleaning).¬†\n‚Ä¢Tuned TF-IDF parameters and applied Bayesian optimization on XGBoost and \nNeural Nets.¬†\n‚Ä¢Visualized key patterns with word clouds and distribution plots.¬†\n‚Ä¢Deployed an interactive real-time Streamlit app for prediction.\nGitHub",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 30
    }
  },
  {
    "id": "9f545c77-5802-400f-9593-d8cdbbacb755",
    "content": "‚Ä¢Deployed an interactive real-time Streamlit app for prediction.\nGitHub\nVehicle Tracker and Counter using YOLOv8 + Deep SORT\nF E B R U A RY  2 0 2 5 \n‚Ä¢Built a real-time vehicle tracking and counting system using YOLOv8 and Deep \n¬†SORT.¬†\n‚Ä¢Used OpenCV for video processing and interactive virtual line logic.¬†\n‚Ä¢Enabled consistent ID tracking across frames and counted unique vehicles \nacross zones.¬†\n‚Ä¢Processed real street surveillance datasets for traffic monitoring use cases.\nGitHub\nCertificates",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 31
    }
  },
  {
    "id": "55a12086-c17e-4a69-8553-ad36143e21f0",
    "content": "‚Ä¢Processed real street surveillance datasets for traffic monitoring use cases.\nGitHub\nCertificates\nData Engineering Zoomcamp, DataTalksClub\nA P R I L  2 0 2 5 \ndbt Fundamentals, dbt Labs\nF E B R U A RY  2 0 2 5 \nWeb Design for Everybody: Basics of Web Development & Coding \nSpecialization, University of Michigan (Coursera)\nS E P T E M B E R  2 0 2 3 \nInterests\n‚Ä¢College soccer team player and self-taught guitar hobbyist.\n‚Ä¢For my non-technical background and skills, including current and interim",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 32
    }
  },
  {
    "id": "ee0dfb5a-9abb-4384-94c5-3959867f0520",
    "content": "‚Ä¢For my non-technical background and skills, including current and interim \nactivities, please visit my Indeed profile: Indeed\nConsent\nI consent to the processing and storage of my personal data for recruitment \npurposes in accordance with applicable data protection laws, including \nconsideration for current and future employment opportunities.",
    "metadata": {
      "source_filename": "Resume2025_New-Joshua.pdf",
      "original_filepath": "uploaded_docs\\Resume2025_New-Joshua.pdf",
      "file_type": "pdf",
      "chunk_index": 33
    }
  }
]